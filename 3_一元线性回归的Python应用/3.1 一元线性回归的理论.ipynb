{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af9ce21-c654-425c-9db9-8c7725cdf020",
   "metadata": {},
   "source": [
    "# 一元线性回归的理论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e3b68-4faa-45ed-b2c7-a5c1c4a7f4df",
   "metadata": {},
   "source": [
    "## 1. 相关系数的理论\n",
    "我们先考虑下两个连续变量之间的统计关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d1d4b-930a-43e0-8b11-7824aeceaa43",
   "metadata": {},
   "source": [
    "![simple_regression](image/simple_regression.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30daf9f6-597b-4490-bbbf-497ec755d304",
   "metadata": {
    "tags": []
   },
   "source": [
    "在线性回归分析开始前，一般计算自变量和因变量的相关系数，我们把这一步称之为相关分析。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497411f5-4f13-4615-8c14-2f868246a3a9",
   "metadata": {},
   "source": [
    "相关系数r检验y和x两个变量之间的线性相关的显著程度，其算式为\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum{(x_i-\\bar{x})^2\\sum{(y_i-\\bar{y})^2}}}}\n",
    "$$\n",
    "数学上可以证明：r在[-1, 1]范围，有：\n",
    "- r>0时，y与x有一定的正线性相关，越接近1正的相关性越大\n",
    "- r<0时，y与x有一定的负线性相关，越接近-1负的相关性越大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5814924a-12de-4720-ae89-2baa94315fae",
   "metadata": {},
   "source": [
    "## 2. 一元线性回归的理论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00215424-8265-4fbf-b63e-33cdb2638382",
   "metadata": {},
   "source": [
    "**输出变量Y**被称为被解释变量、因变量、结果，而**输入变量X**可以被称为解释变量、自变量、效果、预测因子。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b766151-ecd5-4f90-bbb5-19fe47efd6aa",
   "metadata": {},
   "source": [
    "$$\n",
    "Y_i = \\beta_0+\\beta_1 X_i + u_i \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ca5d53-7203-4157-b2bc-5cc267582b73",
   "metadata": {},
   "source": [
    "$$\n",
    "i是第i次观测，i=1,2,...,n;Y_i是被解释变量，\\beta_0是截距；\\beta_1是总体回归线的斜率，u_i是误差项\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e597f98-3995-4ca5-b43c-359f246f15b1",
   "metadata": {},
   "source": [
    "线性回归拟合一个具有系数的线性模型，以最小化数据集内观测目标与线性逼近预测目标之间的残差平方和。数学上，它解决了这样一个问题:\n",
    "\n",
    "$$\n",
    "min\\{\\sum^{n}_{i=1}(Y-\\beta_0-\\beta_1X_i)^2\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5cf8c8-0d9e-4f3b-aedb-84db0cc0a067",
   "metadata": {},
   "source": [
    "### OLS方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ef3b0-708b-4976-8e0a-cbab78759a65",
   "metadata": {},
   "source": [
    "![OLS](image/OLS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14d1b4-d7d4-450c-b5f3-f7e2a3e62f24",
   "metadata": {},
   "source": [
    "为了最小化预测误差平方和$\\sum^{n}_{i=1}(Y-\\beta_0-\\beta_1X_i)^2$，首先将该式关于$b_0$和$b_1$求偏导数，可以得到以下两个等式：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sum(Y_i-\\beta_0-\\beta_1X_i)^2}{\\partial\\beta_0}\n",
    "= -2\\sum(Y_i-\\beta_0-\\beta_1X_i)^2\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\sum(Y_i-\\beta_0-\\beta_1X_i)^2}{\\partial\\beta_1}\n",
    "= -2\\sum(Y_i-\\beta_0-\\beta_1X_i)X_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a350dc-4c41-44f9-a313-595e7ed5d320",
   "metadata": {},
   "source": [
    "令上面2个偏导数等于零，整理后得到OLS估计量$\\beta_0$和$\\beta_1$必须满足的两个方程：\n",
    "\n",
    "$$\n",
    "\\bar{Y}-\\hat{\\beta_0}-\\hat{\\beta_1}\\bar{X}=0\n",
    "$$\n",
    "$$\n",
    "\\frac{1}{n}\\sum{X_i}{Y_i} - \\hat{\\beta_0}\\bar{X}-\\hat{\\beta_1}\\frac{1}{n}\\sum^n_{i=1}X^2_i = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce13a76-b205-4ef8-819f-64fb1fa39fbc",
   "metadata": {},
   "source": [
    "解上述关于$\\hat{\\beta_0}$和$\\hat{\\beta_1}$的方程组，得到\n",
    "\n",
    "$$\n",
    "\\hat{\\beta_1}=\\frac{\\frac{1}{n}\\sum_{i=1}^n X_i Y_i-\\bar{XY}}{\\frac{1}{n}\\sum_{i=1}^n X_i^2 - \\bar{X}^2}\n",
    "$$\n",
    "$$\n",
    "\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2372a-09bc-4f4f-8d7e-3f78d75e4e3b",
   "metadata": {},
   "source": [
    "\n",
    "## 回归模型评价指标\n",
    "\n",
    "### SER\n",
    "\n",
    "回归标准误（SER）是对误差项$\\mu_i$标准差的估计，故SER可用来度量$Y$的分布在回归线周围的离散程度。\n",
    "$$\n",
    "SER = s_\\hat{u}=\\sqrt{s^2_\\hat{u}}\n",
    "$$\n",
    "\n",
    "### $R^2$\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{ESS}{TSS}=\\frac{\\sum^n_{i=1}{(\\hat{Y_i}-\\bar{Y})^2}}{\\sum^n_{i=1}{(Y_i-\\bar{Y})^2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{ESS}{TSS}=1-\\frac{SSR}{TSS}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R^2 = 1-\\frac{SSR}{TSS}\n",
    "$$\n",
    "$$\n",
    "SSR为残差平方和\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### 调整$R^2$\n",
    "\n",
    "$$\n",
    "\\bar{R}^2=1-\\frac{n-1}{n-k-1}\\frac{SSR}{TSS}=1-\\frac{s^2_\\hat{u}}{s^2_Y} \n",
    "$$\n",
    "$$\n",
    "k:斜率系数和截距的数量\n",
    "$$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
